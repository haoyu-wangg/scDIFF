{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5405f5c1",
   "metadata": {},
   "source": [
    "\n",
    "| Dataset |\n",
    "|------------|\n",
    "| BoneMarrowA |\n",
    "| BoneMarrowB |\n",
    "| LungA |\n",
    "| LungB |\n",
    "| WholeBrainA |\n",
    "| WholeBrainB |\n",
    "| LargeIntestineA |\n",
    "| LargeIntestineB |\n",
    "| Cerebellum |\n",
    "| PreFrontalCortex |\n",
    "| SmallIntestine |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Input file paths (change as needed)\n",
    "cell_metadata_path = \"/home/daozhang/Data/raw/GEO_data/mouse-atac/cell_metadata.txt\"  # Contains cell metadata\n",
    "peaks_path = \"/home/daozhang/Data/raw/GEO_data/mouse-atac/atac_matrix.binary.qc_filtered.peaks.txt\"  # Contains peak information\n",
    "cells_path = \"/home/daozhang/Data/raw/GEO_data/mouse-atac/atac_matrix.binary.qc_filtered.cells.txt\"  # Contains all cells list\n",
    "matrix_path = \"/home/daozhang/Data/raw/GEO_data/mouse-atac/atac_matrix.binary.qc_filtered.mtx.gz\"  # Contains sparse matrix file path\n",
    "\n",
    "# Output directory for h5ad files (change as needed)\n",
    "output_dir = \"./Temp\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Tissue mapping dictionary (original tissue.replicate to dataset name)\n",
    "tissue_mapping = {\n",
    "    #\"BoneMarrow_62016\": \"BoneMarrowA\",\n",
    "    \"BoneMarrow_62216\": \"BoneMarrowB\",\n",
    "    # \"Lung1_62216\": \"LungA\",\n",
    "    # \"Lung2_62216\": \"LungB\",\n",
    "    # \"WholeBrainA_62216\": \"WholeBrainA\",\n",
    "    # \"WholeBrainA_62816\": \"WholeBrainB\",\n",
    "    # \"LargeIntestineA_62816\": \"LargeIntestineA\",\n",
    "    # \"LargeIntestineB_62816\": \"LargeIntestineB\",\n",
    "    # \"Cerebellum_62216\": \"Cerebellum\",\n",
    "    # \"PreFrontalCortex_62216\": \"PreFrontalCortex\"\n",
    "    # \"SmallIntestine_62816\": \"SmallIntestine\",\n",
    "}\n",
    "\n",
    "# Step 1: Read cell metadata\n",
    "print(\"Reading cell metadata...\")\n",
    "cell_metadata = pd.read_csv(cell_metadata_path, sep=\"\\t\")\n",
    "\n",
    "# Step 2: Read all cells list\n",
    "print(\"Reading cells list...\")\n",
    "cells = pd.read_csv(cells_path, header=None, names=[\"cell\"])\n",
    "\n",
    "# Step 3: Read peak data\n",
    "print(\"Reading peak information...\")\n",
    "peaks = pd.read_csv(peaks_path, header=None, names=[\"peak\"])\n",
    "\n",
    "# Step 4: Parse peak information\n",
    "print(\"Parsing peak information...\")\n",
    "peaks_split = peaks[\"peak\"].str.split(\"_\", expand=True)\n",
    "peaks_split.columns = [\"chrom\", \"start\", \"end\"]\n",
    "peaks_split[\"chromStart\"] = peaks_split[\"start\"]\n",
    "peaks_split[\"chromEnd\"] = peaks_split[\"end\"]\n",
    "peaks_split[\"chr\"] = peaks_split[\"chrom\"]\n",
    "\n",
    "# Step 5: Load sparse matrix\n",
    "print(\"Loading sparse matrix...\")\n",
    "matrix = scipy.io.mmread(matrix_path).tocsr()\n",
    "matrix = matrix.transpose()\n",
    "\n",
    "# Process each tissue separately\n",
    "for original_tissue, dataset_name in tissue_mapping.items():\n",
    "    print(f\"Processing {original_tissue} -> {dataset_name}...\")\n",
    "    \n",
    "    # Filter metadata for the current tissue\n",
    "    filtered_metadata = cell_metadata[cell_metadata[\"tissue.replicate\"] == original_tissue]\n",
    "    \n",
    "    # Skip if no cells found for this tissue\n",
    "    if filtered_metadata.empty:\n",
    "        print(f\"No cells found for {original_tissue}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Add Batch information based on dataset name\n",
    "    filtered_metadata.loc[:, \"Batch\"] = dataset_name\n",
    "    \n",
    "    # Get selected cells and their indices\n",
    "    selected_cells = filtered_metadata[\"cell\"].tolist()\n",
    "    filtered_cells_idx = cells[cells[\"cell\"].isin(selected_cells)].index\n",
    "    \n",
    "    # Filter matrix for selected cells\n",
    "    filtered_matrix = matrix[filtered_cells_idx, :]\n",
    "    \n",
    "    # Calculate n_cells for each peak (number of cells with accessible chromatin)\n",
    "    print(f\"Calculating n_cells for {dataset_name}...\")\n",
    "    n_cells_per_peak = (filtered_matrix > 0).sum(axis=0).A1  # Count non-zero values per peak\n",
    "    \n",
    "    # Add n_cells to peak information\n",
    "    peaks_split_copy = peaks_split.copy()\n",
    "    peaks_split_copy[\"n_cells\"] = n_cells_per_peak\n",
    "    \n",
    "    # Construct var table\n",
    "    var = peaks_split_copy[[\"chrom\", \"chromStart\", \"chromEnd\", \"chr\", \"start\", \"end\", \"n_cells\"]]\n",
    "    \n",
    "    # Create AnnData object\n",
    "    adata = ad.AnnData(X=filtered_matrix)\n",
    "    \n",
    "    # Add obs (cell metadata)\n",
    "    adata.obs = filtered_metadata.set_index(\"cell\")\n",
    "    \n",
    "    # Add var (peak data)\n",
    "    # 使用原始峰名称作为索引\n",
    "    adata.var = var\n",
    "    adata.var.index = peaks[\"peak\"]  # 使用原始的 \"chr_start_end\" 格式\n",
    "    \n",
    "    # Add CellType column (copy from cell_label)\n",
    "    adata.obs[\"CellType\"] = adata.obs[\"cell_label\"]\n",
    "    \n",
    "    adata.obs[\"batch\"] = 0  # Default to 0\n",
    "    \n",
    "    # Add n_genes column (count of non-zero features per cell)\n",
    "    adata.obs[\"n_genes\"] = (adata.X > 0).sum(axis=1).A1\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Dataset {dataset_name} statistics:\")\n",
    "    print(f\"  - Number of cells: {adata.n_obs}\")\n",
    "    print(f\"  - Number of peaks: {adata.n_vars}\")\n",
    "    print(f\"  - Average peaks per cell: {adata.obs['n_genes'].mean():.1f}\")\n",
    "    print(f\"  - Average cells per peak: {adata.var['n_cells'].mean():.1f}\")\n",
    "    print(f\"  - Peak accessibility range: {adata.var['n_cells'].min()} - {adata.var['n_cells'].max()} cells\")\n",
    "    \n",
    "    # Save AnnData object\n",
    "    output_file = os.path.join(output_dir, f\"{dataset_name}.h5ad\")\n",
    "    print(f\"Saving {dataset_name} dataset to {output_file}...\")\n",
    "    adata.write(output_file)\n",
    "    \n",
    "    print(f\"Completed processing {dataset_name} with {len(filtered_cells_idx)} cells\\n\")\n",
    "\n",
    "print(\"All tissue-specific h5ad files have been generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb1f35",
   "metadata": {},
   "source": [
    "| Dataset |\n",
    "|------------|\n",
    "| MosA1 |\n",
    "| MosA2 |\n",
    "| MosM1 |\n",
    "| MosM2 |\n",
    "| MosP1 |\n",
    "| MosP2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def process_snap_file(file_path, batch_name, csv_file, output_dir):\n",
    "    \"\"\"\n",
    "    Process SNAP (Single Nucleus ATAC-seq) HDF5 file and convert to AnnData format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the input HDF5 file\n",
    "    batch_name : str\n",
    "        Batch identifier for the dataset\n",
    "    csv_file : str\n",
    "        Path to CSV file containing barcode to celltype mapping\n",
    "    output_dir : str\n",
    "        Directory to save the output H5AD file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open HDF5 file and extract data\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get PM (Peak Matrix) group data\n",
    "        pm_group = f['PM']\n",
    "        \n",
    "        # Read sparse matrix components\n",
    "        count_data = pm_group['count'][:]    # Non-zero element values\n",
    "        idx = pm_group['idx'][:]             # Row indices of non-zero elements\n",
    "        idy = pm_group['idy'][:]             # Column indices of non-zero elements\n",
    "        \n",
    "        # Get cell and peak information\n",
    "        cell_barcodes = list(f['BD']['name'][:])          # Cell barcodes\n",
    "        peak_chrom = list(f['PM']['peakChrom'][:])        # Peak chromosome names\n",
    "        peak_start = list(f['PM']['peakStart'][:])        # Peak start positions\n",
    "        peak_end = list(f['PM']['peakEnd'][:])            # Peak end positions\n",
    "        \n",
    "        # Convert byte strings to regular strings\n",
    "        cell_barcodes = [name.decode('utf-8') for name in cell_barcodes]\n",
    "        peak_chrom = [chrom.decode('utf-8') for chrom in peak_chrom]\n",
    "        \n",
    "        # Convert indices from 1-based to 0-based indexing\n",
    "        idx = idx - 1\n",
    "        idy = idy - 1\n",
    "        \n",
    "        # Ensure indices are integer type (required for sparse matrices)\n",
    "        idx = idx.astype(int)\n",
    "        idy = idy.astype(int)\n",
    "        \n",
    "        # Create sparse matrix in COO format\n",
    "        count_matrix = sp.coo_matrix((count_data, (idx, idy)))\n",
    "    \n",
    "    # Create AnnData object\n",
    "    print(\"Creating AnnData object...\")\n",
    "    adata = sc.AnnData(count_matrix)\n",
    "    \n",
    "    # Set cell and peak annotations\n",
    "    adata.obs['cell'] = cell_barcodes\n",
    "    adata.var['chr'] = peak_chrom\n",
    "    adata.var['start'] = peak_start\n",
    "    adata.var['end'] = peak_end\n",
    "    \n",
    "    # Load barcode to celltype mapping\n",
    "    print(\"Loading cell type annotations...\")\n",
    "    barcode_df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Ensure barcode columns are string type\n",
    "    barcode_df['barcode'] = barcode_df['barcode'].astype(str)\n",
    "    adata.obs['cell'] = adata.obs['cell'].astype(str)\n",
    "    \n",
    "    # Create barcode to celltype mapping dictionary\n",
    "    barcode_to_celltype = dict(zip(barcode_df['barcode'], barcode_df['celltype']))\n",
    "    \n",
    "    # Map cell types to observations\n",
    "    adata.obs['CellType'] = adata.obs['cell'].map(barcode_to_celltype)\n",
    "    adata.obs['Batch'] = batch_name\n",
    "    \n",
    "    # Convert sparse matrix to CSR format for efficiency\n",
    "    adata.X = csr_matrix(adata.X)\n",
    "    \n",
    "    # Save processed data\n",
    "    output_path = f\"{output_dir}/{batch_name}.h5ad\"\n",
    "    print(f\"Saving processed data to {output_path}\")\n",
    "    adata.write(output_path)\n",
    "    \n",
    "    print(f\"Processing complete! AnnData shape: {adata.shape}\")\n",
    "    return adata\n",
    "\n",
    "\n",
    "# Define file paths and parameters (change as needed)\n",
    "# GSM3611836 snATAC_MOs_A_rep1\n",
    "# GSM3611837 snATAC_MOs_A_rep2\n",
    "# GSM3611838 snATAC_MOs_M_rep1\n",
    "# GSM3611839 snATAC_MOs_M_rep2\n",
    "# GSM3611840 snATAC_M0s_P_rep1\n",
    "# GSM3611841 snATAC_MOs_P_rep2\n",
    "\n",
    "\n",
    "# Define file paths and parameters (change as needed)\n",
    "file_path = \"/home/daozhang/Data/raw/GEO_data/GSE126724/GSM3611840_CEMBA180308_3B.snap.hdf5\"\n",
    "batch_name = \"MosP1\"\n",
    "csv_file = \"/home/daozhang/Data/raw/GEO_data/GSE126724/41467_2021_21583_MOESM4_ESM.csv\"\n",
    "output_dir = \"./Temp\"\n",
    "\n",
    "# Process the data\n",
    "adata = process_snap_file(file_path, batch_name, csv_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff85aa",
   "metadata": {},
   "source": [
    "| Dataset |\n",
    "|------------|\n",
    "| KidneyA |\n",
    "| KidneyB |\n",
    "| KidneyC |\n",
    "| KidneyD |\n",
    "| KidneyE |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "# Sample UUID to batch name mapping (5 samples)\n",
    "sample_uuid_mapping = {\n",
    "    '8628f96c-2d97-4579-951c-044017ef6f0e': 'KidneyA',\n",
    "    '84824920-47bc-4929-b123-87574944aa37': 'KidneyB',\n",
    "    'c47a5959-a0ed-4276-8a8d-5f136f1edd29': 'KidneyC', \n",
    "    'd8b737fb-8a38-4a1d-a421-cbc81edddd05': 'KidneyD',\n",
    "    'e7372715-150e-4117-a11b-a88ea56ce5c5': 'KidneyE'\n",
    "}\n",
    "\n",
    "def process_anndata_to_h5ad(adata):\n",
    "    \"\"\"\n",
    "    Process AnnData object to generate standardized h5ad file\n",
    "    \"\"\"\n",
    "    # Create a copy of AnnData\n",
    "    adata_processed = adata.copy()\n",
    "    \n",
    "    # 1. Process obs data - keep only essential columns\n",
    "    # Create new Batch column based on sample_uuid mapping\n",
    "    adata_processed.obs['Batch'] = adata_processed.obs['sample_uuid'].map(sample_uuid_mapping)\n",
    "    \n",
    "    # Add CellType column (using existing cell_type column)\n",
    "    adata_processed.obs['CellType'] = adata_processed.obs['cell_type']\n",
    "    \n",
    "    # Keep only essential obs columns\n",
    "    essential_obs_cols = ['Batch', 'CellType']\n",
    "    adata_processed.obs = adata_processed.obs[essential_obs_cols]\n",
    "    \n",
    "    # 2. Process var data - preserve genomic region information\n",
    "    # Rename var columns to match requirements\n",
    "    var_mapping = {\n",
    "        'chrom': 'chr',\n",
    "        'chromStart': 'start', \n",
    "        'chromEnd': 'end'\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    adata_processed.var = adata_processed.var.rename(columns=var_mapping)\n",
    "    \n",
    "    # Keep only essential var columns\n",
    "    essential_var_cols = ['chr', 'start', 'end']\n",
    "    adata_processed.var = adata_processed.var[essential_var_cols]\n",
    "    \n",
    "    # 3. Filter peaks - remove peaks accessible in <1% of cells\n",
    "    # Calculate peak accessibility across cells\n",
    "    peak_accessibility = (adata_processed.X > 0).sum(axis=0)\n",
    "    if hasattr(peak_accessibility, 'A1'):  # Handle sparse matrix\n",
    "        peak_accessibility = peak_accessibility.A1\n",
    "    \n",
    "    # Calculate threshold (1% of cells)\n",
    "    min_cells_threshold = int(0.01 * adata_processed.n_obs)\n",
    "    \n",
    "    # Create filter mask\n",
    "    peaks_to_keep = peak_accessibility >= min_cells_threshold\n",
    "    \n",
    "    print(f\"Total peaks: {adata_processed.n_vars}\")\n",
    "    print(f\"Min cells threshold (1%): {min_cells_threshold}\")\n",
    "    print(f\"Peaks retained after filtering: {np.sum(peaks_to_keep)}\")\n",
    "    print(f\"Peaks filtered out: {np.sum(~peaks_to_keep)}\")\n",
    "    \n",
    "    # Apply filtering\n",
    "    adata_processed = adata_processed[:, peaks_to_keep]\n",
    "    \n",
    "    # 4. Clean unnecessary metadata\n",
    "    # Clean obsm (keep only essential)\n",
    "    if 'X_umap' in adata_processed.obsm:\n",
    "        adata_processed.obsm = {'X_umap': adata_processed.obsm['X_umap']}\n",
    "    else:\n",
    "        adata_processed.obsm = {}\n",
    "    \n",
    "    # Clean uns (keep only basic info)\n",
    "    essential_uns = {}\n",
    "    if 'title' in adata_processed.uns:\n",
    "        essential_uns['title'] = adata_processed.uns['title']\n",
    "    adata_processed.uns = essential_uns\n",
    "    \n",
    "    return adata_processed\n",
    "\n",
    "\n",
    "\n",
    "# Load data (change as needed)\n",
    "adata = sc.read_h5ad('/home/daozhang/Data/raw/Muto-2021-ATAC.h5ad') \n",
    "# Process data\n",
    "adata_processed = process_anndata_to_h5ad(adata)\n",
    "# Save processed file\n",
    "adata_processed.write_h5ad('./Temp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86d59f",
   "metadata": {},
   "source": [
    "| Dataset |\n",
    "|------------|\n",
    "| MouseBrain(10X) |\n",
    "| NormalCortex |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe80e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.io import mmread\n",
    "import os\n",
    "\n",
    "def create_and_filter_anndata(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Create and filter AnnData object from files in the specified directory\n",
    "    \n",
    "    Parameters:\n",
    "    input_dir (str): Directory containing input files\n",
    "    output_file (str): Path to save the filtered h5ad file\n",
    "    \n",
    "    Returns:\n",
    "    anndata.AnnData: Filtered AnnData object\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    barcode_file = os.path.join(input_dir, 'barcodes.tsv')\n",
    "    peaks_file = os.path.join(input_dir, 'peaks.bed')\n",
    "    matrix_file = os.path.join(input_dir, 'matrix.mtx')\n",
    "    \n",
    "    # Read barcodes\n",
    "    with open(barcode_file, 'r') as f:\n",
    "        barcodes = [line.strip() for line in f]\n",
    "    \n",
    "    # Read peaks\n",
    "    peaks = pd.read_csv(peaks_file, sep='\\t', header=None,\n",
    "                       names=['chr', 'start', 'end'])\n",
    "    \n",
    "    # Read sparse matrix using mmread and transpose it\n",
    "    data = mmread(matrix_file).tocsr().T\n",
    "    \n",
    "    # Calculate n_genes (number of non-zero peaks per cell)\n",
    "    n_genes = np.asarray(data.sum(axis=1)).flatten()\n",
    "    \n",
    "    # Calculate n_cells (number of cells with non-zero counts per peak)\n",
    "    n_cells = np.asarray(data.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Create obs DataFrame\n",
    "    obs = pd.DataFrame(index=barcodes)\n",
    "    obs['cell'] = barcodes\n",
    "    obs['tissue'] = 'NormalCortex'\n",
    "    obs['Batch'] = 'NormalCortex'\n",
    "    obs['batch'] = ['1'] * len(barcodes)\n",
    "    obs['n_genes'] = n_genes\n",
    "    \n",
    "    # Create var DataFrame with formatted peak names\n",
    "    var = pd.DataFrame(index=range(len(peaks)))\n",
    "    var['chr'] = peaks['chr']\n",
    "    var['start'] = peaks['start']\n",
    "    var['end'] = peaks['end']\n",
    "    var['n_cells'] = n_cells\n",
    "    \n",
    "    # Create formatted peak names\n",
    "    var.index = var.apply(lambda row: f\"{row['chr']}_{row['start']}_{row['end']}\", axis=1)\n",
    "    \n",
    "    # Create initial AnnData object\n",
    "    adata = ad.AnnData(X=data,\n",
    "                      obs=obs,\n",
    "                      var=var)\n",
    "    \n",
    "    print(\"Initial AnnData:\")\n",
    "    print(f\"Matrix shape: {data.shape}\")\n",
    "    print(f\"Number of barcodes: {len(barcodes)}\")\n",
    "    print(f\"Number of peaks: {len(peaks)}\")\n",
    "    print(adata)\n",
    "    \n",
    "    # Filter cells and peaks\n",
    "    cells_mask = adata.obs['n_genes'] > 0\n",
    "    peaks_mask = adata.var['n_cells'] > 0\n",
    "    \n",
    "    # Create filtered AnnData object\n",
    "    adata_filtered = adata[cells_mask, peaks_mask].copy()\n",
    "    \n",
    "    print(\"\\nFiltering results:\")\n",
    "    print(f\"Removed {np.sum(~cells_mask)} cells with no expression\")\n",
    "    print(f\"Removed {np.sum(~peaks_mask)} peaks with no expression\")\n",
    "    print(f\"Final dimensions: {adata_filtered.shape}\")\n",
    "    \n",
    "    # Save the filtered AnnData object\n",
    "    adata_filtered.write_h5ad(output_file)\n",
    "    print(f\"\\nSaved filtered AnnData to: {output_file}\")\n",
    "    \n",
    "    return adata_filtered\n",
    "\n",
    "\n",
    "# Set input and output paths (change as needed)\n",
    "input_directory = \"/home/daozhang/Data/raw/Normal_cortex\"\n",
    "output_file = \"./Temp/NormalCortex.h5ad\"\n",
    "\n",
    "# Create and filter AnnData\n",
    "adata = create_and_filter_anndata(input_directory, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SANGO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
